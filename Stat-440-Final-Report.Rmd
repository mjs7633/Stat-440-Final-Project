---
title: "Stat 440 Final Project"
author: "Matthew Secen"
date: "11/30/2021"
output: html_document
---

## Front Matter

```{r setup, include=FALSE}
rm(list = ls())
library("readxl")
library("dplyr")
library("tidyr")
library("ggplot2")
library("mosaic")
library("stats")
library("rpart")
#install.packages("partykit")
library("partykit")
library("randomForest")
library("tibble")
library("gbm")
set.seed(440)
```

```{r}
Happy2019_raw<- read.csv("2019.csv")
head(Happy2019_raw)

Happy2018_raw<- read.csv("2018.csv")
head(Happy2018_raw)
```


## Clean Data

```{r}
Happy2019 <-
  Happy2019_raw %>%
  rename(happiness_rank = Overall.rank,
    country = Country.or.region,
    score = Score,
    gdp_per_capita = GDP.per.capita,
    social_support = Social.support,
    life_expectancy = Healthy.life.expectancy,
    freedom = Freedom.to.make.life.choices,
    generosity = Generosity,
    corruption = Perceptions.of.corruption
) %>%
  mutate(happy = as.factor(ifelse(score >= 5, "yes", "no")),
         wealthy = as.factor(ifelse(gdp_per_capita >= 1, "yes", "no")))
Happy2019


Happy2018 <-
  Happy2018_raw %>%
  rename(happiness_rank = Overall.rank,
    country = Country.or.region,
    score = Score,
    gdp_per_capita = GDP.per.capita,
    social_support = Social.support,
    life_expectancy = Healthy.life.expectancy,
    freedom = Freedom.to.make.life.choices,
    generosity = Generosity,
    corruption = Perceptions.of.corruption
) %>%
  mutate(happy = as.factor(ifelse(score >= 5, "yes", "no")),
         wealthy = as.factor(ifelse(gdp_per_capita >= 1, "yes", "no")))

Happy2018
```


## Partition the Data
```{r}
set.seed(440)

n <-nrow(Happy2019)
test_idx <- sample.int(n, size = round(.25*n))

train <- Happy2019[-test_idx,]
nrow(train)
test <- Happy2019[test_idx,]
nrow(test)
train

#pairs(train)
```


### Null Model
```{r}
mod_null <- tally(~ happy, data = train)
mod_null
```



### Decision Tree (simple)
```{r}
rpart(happy ~ gdp_per_capita, data = train)
```

```{r}
split <- .675 # first split from simple `rpart`

train %>%
  mutate(elevated_happiness = gdp_per_capita >= split) %>%
  ggplot(aes(x = gdp_per_capita, y = happy)) + 
  geom_point(aes(color = elevated_happiness, alpha =.04)) + 
  geom_vline(xintercept = split, color = "blue", lty = 2)
```



### Decision Tree (Complete)
```{r}
mod_tree <- rpart(happy ~ gdp_per_capita + social_support + life_expectancy + freedom + generosity + corruption, data = train)
mod_tree
```

```{r}
plot(as.party(mod_tree))
```

```{r}
printcp(mod_tree)
```

```{r}
train_tree <- 
  train %>%
  mutate(happy_dtree = predict(mod_tree, type = "class"))

confusion <- tally(happy_dtree ~ happy, data = train_tree, format = "count")
confusion

dtree_acc <- sum(diag(confusion)) / nrow(train) * 100
dtree_acc
```

### Random Forrest

```{r}
mod_forest <- randomForest(happy ~ gdp_per_capita + social_support + life_expectancy + freedom + generosity + corruption, data = train, ntree = 2000, mtry = 2)
mod_forest

rf_acc <- sum(diag(mod_forest$confusion)) / nrow(train) * 100
rf_acc
```

```{r}
importance(mod_forest) %>%
  as.data.frame() %>%
  rownames_to_column() %>%  # handy function from `tibble` package
  arrange(desc(MeanDecreaseGini))
```

```{r}
# response must be converted to 0/1
train_boost <- 
  train %>%
  mutate(happy01 = if_else(happy == "yes", 1, 0)) %>%
  select(-happy)
mod_boost <- gbm(happy01 ~ gdp_per_capita + social_support + life_expectancy + freedom + generosity + corruption, distribution = "bernoulli",   # because it's a classifier model
                     data = train_boost, n.trees = 3000, interaction.depth = 2) 
# the relative influence is similar to importance result earlier
msummary(mod_boost)
```



### K-Nearest Neighbor

```{r}
#install.packages('mdsr')
library(mdsr)
library(tidyverse)
library(lubridate)
library(mosaic)
library(rvest)
library(methods)
library(readr)
#install.packages('randomForest')
library(randomForest)
library(tibble)
library(class)
#install.packages('e1071')
library(e1071)
library(nnet)
```


```{r}
require(class)
Happy2019$gdp_per_capita<-as.numeric(as.integer(factor(Happy2019$gdp_per_capita)))
train_quant <- 
  Happy2019 %>%
  select(gdp_per_capita)
# KNN classifier


happy_knn <- knn(train = train_quant, test = train_quant, cl = Happy2019$happy, k = 5)
# confusion matrix

confusion <- tally(happy_knn ~ happy, data = Happy2019, format = "count")


knn_acc <- sum(diag(confusion)) / nrow(train_quant) * 100
knn_acc
```

```{r}
#Naive Bayes
head(train, 1)
train

tally(wealthy ~ happy, data = train, margins = TRUE)
tally( ~ wealthy, data = train, margins = TRUE)
tally( ~ happy, data = train, margins = TRUE)

require(e1071)  # awful name for a package...

mod_nb <- naiveBayes(happy ~ ., data = train)
happy_nb <- predict(mod_nb, newdata = train)
confusion <- tally(happy_nb ~ happy, data = train, format = "count")
confusion

nb_acc <- sum(diag(confusion)) / nrow(train) * 100
nb_acc
```

```{r}
#Artificial Neural Networks
require(nnet)
mod_nnet <- nnet(happy ~ ., data = train, size = 7)
happy_nn <- predict(mod_nnet, newdata = train, type = "class")
confusion <- tally(happy_nn ~ happy, data = train, format = "count")
confusion

nnet_acc <- sum(diag(confusion)) / nrow(train) * 100
nnet_acc
```



### Logistic Regression Model


```{r}
# logistic
mod_logit <- glm(happy ~ gdp_per_capita, data = train, family = 'binomial')
msummary(mod_logit)

happy_logitProb <- predict(mod_logit, newdata = train, type = 'response')
happy_logit <- ifelse(happy_logitProb > 0.5, yes = 'yes', 'no')
confusion <- tally(happy_logit ~ happy, data = train, format = 'count')


logit_acc <- sum(diag(confusion)) / nrow(train) * 100
logit_acc

#Trying
```


```{r}
ModelComparison <- 
  tribble(
  ~model, ~accuracy, 
  "**NULL MODEL**", mod_null[1], 
  "decision tree", dtree_acc, 
  "random forest", rf_acc, 
  "k-nearest neighbors", knn_acc, 
  "naive Bayes", nb_acc, 
  "neural network", nnet_acc, 
  "logistic regression", logit_acc
)
ModelComparison %>%
  arrange(desc(accuracy))
```






